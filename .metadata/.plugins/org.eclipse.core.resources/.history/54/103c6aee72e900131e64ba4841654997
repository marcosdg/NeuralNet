package core.learning;

import java.util.ArrayList;
import java.util.List;

import core.Connection;
import core.Layer;
import core.NeuralNetwork;
import core.Neuron;
import core.Weight;

public class Backpropagation
{
	private NeuralNetwork neuralNetwork;
	
	public Backpropagation(NeuralNetwork neuralNetwork)
	{
		this.neuralNetwork = neuralNetwork;
	}
	
	public void forwardPropagation(List<Double> netInput)
	{
		Layer init = this.neuralNetwork.getInitLayer();
		ArrayList<Neuron> neurons = init.getNeurons();
		
		if (netInput.size() != neurons.size()) {
			throw new IllegalArgumentException("Wrong number of input parameters");
		}
		
		/**
		 * This bucle propagates the input sample to the second layer
		 */
		for (int i = 0; i < neurons.size(); i++) {
			neurons.get(i).setNetInput(netInput.get(i));
		}
		
		List<Layer> layers = this.neuralNetwork.getLayers();
		
		/**
		 * Excluding last layer and init layer
		 */
		for(int i = 1; i < layers.size() - 1; i++) {
			neurons = layers.get(i).getNeurons();
			for (Neuron n : neurons) {
				// ini
				n.computeNeuronInput();
				// ai <- g(ini)
				n.computeOutput();
			}
		}
	}
	
	public void backPropagation(List<Double> expectedOutputs) {
		List<Connection> outputs = new ArrayList<Connection>();
		double deltaWeight = 0.0,
				error = 0.0;

		Layer lastLayer = this.neuralNetwork.getOutputLayer();
		ArrayList<Neuron> lastNeurons = lastLayer.getNeurons();
		
		if (expectedOutputs.size() != lastNeurons.size()) {
			throw new IllegalArgumentException("Wrong number of output parameters");
		}
		
		Neuron currentNeuron;
		
		for (int i = 0; i < neurons.size(); i++) {
			currentNeuron = neurons.get(i);
			error = LearningRule.getDelta(currentNeuron, expectedOutputs.get(i));
			currentNeuron.setError(error);
		}
		
		List<Layer> layers = this.neuralNetwork.getLayers();
		List<Connection> connections;
		
		/**
		 * Excluding last layer and init layer
		 */
		for(int i = layers.size() - 1; i < 0; i--) {
			neurons = layers.get(i).getNeurons();
			
			for (Neuron neuron : neurons) {
				
				//g'(ini) * SUM(wji * deltai)
				
				error = LearningRule.getDelta(neuron);
				neuron.setError(error);
				
				//Update weights
				
				outputs = neuron.getOutputs();
				for (Connection output : outputs) {
					
					//n * aj * deltai
					
					deltaWeight = LearningRule.LEARNING_FACTOR * 
                                  neuron.getOutput() *
                                  c.getTarget().getError();
					
					//TODO: Add momentum defined in NeuralNetwork
					Weight updatedWeight = c.getWeight();
					updatedWeight.setValue(updatedWeight.getValue() + deltaWeight);
				}
			}
		}
	}
}
